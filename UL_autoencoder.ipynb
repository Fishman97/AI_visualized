{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb741aea",
   "metadata": {},
   "source": [
    "\n",
    "# Unsupervised Learning: Autoencoder on MNIST\n",
    "\n",
    "This notebook is a demo that trains a simple fully connected autoencoder on MNIST:\n",
    "- Input: **28×28** grayscale (flattened to 784)\n",
    "- Encoder: 784 → 256 → 9\n",
    "- Decoder: 9 → 256 → 784\n",
    "- Loss: Binary Cross Entropy (BCE) on normalized pixels\n",
    "- Optimizer: Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \"cpu\"))\n",
    "use_cuda = (device.type == \"cuda\")\n",
    "pin_memory = True if use_cuda else False\n",
    "non_blocking = True if use_cuda else False\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3655955",
   "metadata": {},
   "source": [
    "\n",
    "## Data\n",
    "We load MNIST, normalize to [0,1], and create train/val loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # [0,1]\n",
    "])\n",
    "\n",
    "data_root = \"./data\"\n",
    "train_dataset_full = datasets.MNIST(root=data_root, train=True, download=True, transform=transform)\n",
    "\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset_full) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset_full, [train_size, val_size])\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "print( \"Number of samples in train dataset:\", len(train_dataset) )\n",
    "print( \"Number of samples in validation dataset:\", len(val_dataset) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12220ba6",
   "metadata": {},
   "source": [
    "\n",
    "## Model\n",
    "A symmetric fully connected autoencoder with a **9-dim latent** (displayed as 3×3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, latent_dim=9):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Encoder (removed 64-unit hidden layer)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, latent_dim),\n",
    "        )\n",
    "        # Decoder (symmetric, no 64-unit hidden layer)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 28*28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        x = x.view(b, -1)\n",
    "        z = self.encoder(x)\n",
    "        xhat = self.decoder(z)\n",
    "        xhat = xhat.view(b, 1, 28, 28)\n",
    "        return xhat, z\n",
    "\n",
    "model = AE(latent_dim=9).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b408000",
   "metadata": {},
   "source": [
    "\n",
    "## Architecture Diagram (schematic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ec4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [\n",
    "    \"784 (28x28)\",\n",
    "    \"256\",\n",
    "    \"9\",\n",
    "    \"256\",\n",
    "    \"784 (28x28)\"]\n",
    "\n",
    "# Horizontal layout: draw layer boxes left -> right and arrows between them\n",
    "plt.figure(figsize=(12, 2))\n",
    "ax = plt.gca()\n",
    "n = len(layers)\n",
    "ax.set_xlim(0, n + 1)\n",
    "ax.set_ylim(0, 1)\n",
    "for i, name in enumerate(layers):\n",
    "    x = i + 1\n",
    "    # rectangle centered vertically\n",
    "    rect_y = 0.2\n",
    "    rect_h = 0.6\n",
    "    ax.add_patch(plt.Rectangle((x - 0.4, rect_y), 0.8, rect_h, fill=False))\n",
    "    ax.text(x, rect_y + rect_h / 2, name, ha='center', va='center', wrap=True)\n",
    "    if i < n - 1:\n",
    "        # arrow pointing right — start at right edge (x+0.4) and end at next box left edge (x+0.6) so dx=0.2\n",
    "        ax.arrow(x + 0.4, 0.5, 0.2, 0, length_includes_head=True, head_width=0.03, head_length=0.03)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.title(\"Autoencoder (Fully Connected)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374530bf",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "We use BCE loss on pixel values in [0,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 10\n",
    "# ensure model parameters are on the chosen device before training\n",
    "model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for x, _ in loader:\n",
    "        # move inputs to same device as model; non_blocking only used for CUDA\n",
    "        x = x.to(device, non_blocking=non_blocking)\n",
    "        xhat, _ = model(x)\n",
    "        loss = criterion(xhat, x)\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        total += loss.item() * x.size(0)\n",
    "        count += x.size(0)\n",
    "    return total / count\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "for ep in range(1, epochs+1):\n",
    "    tr = run_epoch(train_loader, train=True)\n",
    "    vl = run_epoch(val_loader, train=False)\n",
    "    train_losses.append(tr); val_losses.append(vl)\n",
    "    print(f\"Epoch {ep:02d}/{epochs} | train {tr:.4f} | val {vl:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(range(1, epochs+1), train_losses, marker=\"o\", label=\"train\")\n",
    "plt.plot(range(1, epochs+1), val_losses, marker=\"o\", label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"BCE loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2eec58",
   "metadata": {},
   "source": [
    "\n",
    "## Reconstructions and Original → Compressed → Decompressed examples\n",
    "Below: for several validation samples show Original image (left), compressed latent (middle, 3×3 heatmap), and decompressed reconstruction (right).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94725c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "x, _ = next(iter(val_loader))\n",
    "x = x[:16].to(device)\n",
    "with torch.no_grad():\n",
    "    xhat, z = model(x)\n",
    "\n",
    "import matplotlib\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_triplets(orig_batch, z_batch, recon_batch, n=8):\n",
    "    n = min(n, orig_batch.size(0))\n",
    "    orig = orig_batch[:n].cpu()\n",
    "    recon = recon_batch[:n].cpu()\n",
    "    # reshape latent to (n, 3, 3)\n",
    "    z = z_batch[:n].view(-1, 3, 3).cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n, ncols=3, figsize=(6, 2*n))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1,3)\n",
    "    for i in range(n):\n",
    "        ax0 = axes[i,0]\n",
    "        ax1 = axes[i,1]\n",
    "        ax2 = axes[i,2]\n",
    "        # Original\n",
    "        ax0.imshow(orig[i].squeeze(), cmap='gray')\n",
    "        ax0.axis('off')\n",
    "        if i == 0:\n",
    "            ax0.set_title('Original (28x28)')\n",
    "        # Compressed: normalize per-sample and show as 3x3 black & white image (no interpolation blur)\n",
    "        zi = z[i]\n",
    "        zn = (zi - zi.min()) / (zi.max() - zi.min() + 1e-8)\n",
    "        ax1.imshow(zn, cmap='gray', interpolation='nearest', aspect='equal')\n",
    "        ax1.set_xticks([]); ax1.set_yticks([])\n",
    "        if i == 0:\n",
    "            ax1.set_title('Compressed (3×3)')\n",
    "        # Reconstruction\n",
    "        ax2.imshow(recon[i].squeeze(), cmap='gray')\n",
    "        ax2.axis('off')\n",
    "        if i == 0:\n",
    "            ax2.set_title('Reconstruction (28x28)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# show first 8 triplets\n",
    "show_triplets(x, z, xhat, n=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
